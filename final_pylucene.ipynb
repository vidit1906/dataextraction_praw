{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa314c26-2e4d-4f99-a86d-5a80ebd78964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lucene\n",
    "import os\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, FieldType\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from java.nio.file import Paths\n",
    "from lucene import getVMEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa07ca93-cefd-48c1-84e5-bbc55b45001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_reddit_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a89f4a9-166e-4c5d-9e81-468296c00334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "base_dir = 'reddit_lucene_index'\n",
    "def create_index(dir, data):\n",
    "    writer = None\n",
    "    total_time = 0\n",
    "    #num_documents = 0\n",
    "    try:\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        store = SimpleFSDirectory(Paths.get(dir))\n",
    "        analyzer = StandardAnalyzer()\n",
    "        config = IndexWriterConfig(analyzer)\n",
    "        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n",
    "        writer = IndexWriter(store, config)\n",
    "    \n",
    "        \n",
    "        metaType = FieldType()\n",
    "        metaType.setStored(True)\n",
    "        metaType.setTokenized(False)\n",
    "    \n",
    "        contextType = FieldType()\n",
    "        contextType.setStored(True)\n",
    "        contextType.setTokenized(True)\n",
    "        contextType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)\n",
    "    \n",
    "        for sample in data:\n",
    "            start_time=time.time()\n",
    "            subreddit = sample.get('subreddit', '')\n",
    "            post_title = sample.get('post_title', '')\n",
    "            post_body = sample.get('post_body', '')  \n",
    "            parent_comment_id = sample.get('parent_comment_id', '')\n",
    "            comment_id = sample.get('comment_id', '')\n",
    "            comment_body = sample.get('body', '')  \n",
    "    \n",
    "            doc = Document()\n",
    "            doc.add(Field('Subreddit', subreddit, metaType))\n",
    "            doc.add(Field('PostTitle', post_title, metaType))\n",
    "            doc.add(Field('PostBody', post_body, contextType))  \n",
    "            doc.add(Field('ParentCommentId', parent_comment_id, metaType))\n",
    "            doc.add(Field('CommentId', comment_id, metaType))\n",
    "            doc.add(Field('CommentBody', comment_body, contextType))\n",
    "            writer.addDocument(doc)\n",
    "            end_time=time.time()\n",
    "            total_time+=(end_time - start_time)\n",
    "            #num_documents += 1\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "    #avg_time_per_doc = total_time/num_documents if num_documents else 0\n",
    "    print(\"total time taken to index(in seconds):\", total_time )\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f355d5-babf-45c9-b793-b942da5a01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(storedir, query):\n",
    "    searchDir = SimpleFSDirectory(Paths.get(storedir))\n",
    "    searcher = IndexSearcher(DirectoryReader.open(searchDir))\n",
    "    parser = QueryParser('CommentBody', StandardAnalyzer())  \n",
    "    parsed_query = parser.parse(query)\n",
    "\n",
    "    topDocs = searcher.search(parsed_query, 5).scoreDocs\n",
    "    topkdocs = []\n",
    "    for hit in topDocs:\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        topkdocs.append({\n",
    "            \"score\": hit.score,\n",
    "            \"subreddit\": doc.get(\"Subreddit\"),\n",
    "            \"post_title\": doc.get(\"PostTitle\"),\n",
    "            \"post_body\": doc.get(\"PostBody\"),  \n",
    "            \"parent_comment_id\": doc.get(\"ParentCommentId\"),  \n",
    "            \"comment_id\": doc.get(\"CommentId\"),  \n",
    "            \"comment_body\": doc.get(\"CommentBody\")\n",
    "        })\n",
    "    \n",
    "    return topkdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa19ca5-7a54-4daa-9d82-7cb0c439372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time taken to index(in seconds): 55.85971021652222\n",
      "[{'score': 6.4062042236328125, 'subreddit': 'Tronix', 'post_title': 'Tron Protocol Inc. Article of Incorporation. SF Office confirmed', 'post_body': '', 'parent_comment_id': 't1_ds4a4x1', 'comment_id': 'ds4btxk', 'comment_body': 'Search Tron Protocol here https://businesssearch.sos.ca.gov/'}, {'score': 5.895445823669434, 'subreddit': 'OMGnetwork', 'post_title': 'Daily Discussion - February 03, 2021', 'post_body': '## [OMG Network Daily Discussion](https://i.imgur.com/jiwfY8e.png)\\n\\n**Rules**\\n\\n* Please read and follow the subreddit rules in the sidebar.\\n* Please read the [disclaimer](https://omg.eco/disclaimer).\\n\\n**OMG Network Resources**\\n\\n* [Official OMG Network Telegram](https://t.me/OmiseGo)\\n* [What is OMG Network?](https://youtu.be/w-tKyhA4QzE)\\n* [Website](https://omg.network/)\\n* [Network Documentation](https://docs.omg.network/)\\n* [Block Explorer](https://omg.eco/blockexplorer)\\n* [Web Wallet](https://omg.eco/webwallet)\\n* [OMG Network Addresses - Etherscan](https://omg.eco/etherscan)\\n* [Mainnet 2020 Presentation](https://omg.eco/mainnet2020)\\n* [Bug Bounty](https://omg.eco/bugbounty)\\n* [Bug Bounty Terms & Conditions](https://omg.eco/bugbountyterms)\\n\\n[**Recent Blogs**](https://omg.network/blog/)\\n\\n* [OMG Network Integrates With The Ledger Hardware Wallet!](https://omg.network/omg-network-integrates-with-the-ledger-hardware-wallet/)\\n* [3 Reasons Why Blockchain For Central Bank Digital Currency (CBDC)](https://omg.network/3-reasons-why-blockchain-cbdc/)\\n* [Introducing The OMG Network Engineering Blog!](https://omg.network/introducing-omg-network-engineering-blog/)\\n* [Blockchain Central Bank Digital Currency (CBDC) in Asia: Where Weâ€™re Headed](https://omg.network/blockchain-cbdc-in-asia-2020/)\\n* [Why Would Enterprises Use the Public OMG Network?](https://omg.network/why-public-plasma-enterprise/)\\n\\n**CPE (Community Points Engine)**\\n\\n* [ROCKs Leaderboard](https://omgnetwork.rocks/)\\n* [User Guide](https://omg.eco/CPEguide)\\n* [Announcement Blog Post](https://omg.eco/CPEblog)\\n* [Overview Video](https://youtu.be/sopkmwsNm5Q)\\n* [Official Bake-Off Proposal](https://www.reddit.com/r/ethereum/comments/i19us9/omg_networks_great_reddit_scaling_bakeoff_proposal/)\\n* [GitHub Repo](https://omg.eco/CPErepo)\\n* [Reddit Username Registration Thread](https://www.reddit.com/r/OMGnetwork/comments/i0hn9r/ox/)\\n* [CPE Support Thread](https://www.reddit.com/r/OMGnetwork/comments/i2sl1y/omg_community_points_engine_cpe_support_thread/)', 'parent_comment_id': 't1_glt27f6', 'comment_id': 'glt9v32', 'comment_body': '[https://publish.twitter.com/?query=https%3A%2F%2Ftwitter.com%2Fomgnetworkhq%2Fstatus%2F1356079448844460035&widget=Tweet](https://publish.twitter.com/?query=https%3A%2F%2Ftwitter.com%2Fomgnetworkhq%2Fstatus%2F1356079448844460035&widget=Tweet)'}, {'score': 5.782642841339111, 'subreddit': 'Monero', 'post_title': '\"Privacy matters\" starter pack', 'post_body': '', 'parent_comment_id': 't3_7jivc3', 'comment_id': 'dr6pv11', 'comment_body': 'Jesus Christ.  Here, use a garbage search engine cus MUH PRIVACY'}, {'score': 5.697470188140869, 'subreddit': 'NEO', 'post_title': \"I've just filed my crypto taxes, this is what I did...\", 'post_body': '1. Hop on bitcoin.tax and upload all of your trades, if you made over 100 trades in 2017, you will need to purchase the 40ish dollar premium license.\\n\\n2. After you\\'ve uploaded all of your trades into bitcoin.tax, (For upload, you have the option of manual entry, .csv upload, or API link) you will need to save everything as a .txf file. In order to do this you will need to go to Reports & Export =>Download => and finally \"Turbotax CD/Download / H&R Block.\" This will save your 2017 trade history as a .txf file.\\n\\n3. Unfortunately, the only way you can upload your trades to Turbotax is by buying a physical copy of the software. I bought the Premier Version ($70,) it\\'s geared towards those with investments. I have no clue as to how well any other version might work.\\n\\n4. Once you open Turbotax, go to Federal Taxes => Wages & Income => Investment Income => Stocks, Mutual Funds, Bonds, Other. This is where you will be able to upload all your 2017 trades in the form of the .txf file provided by Bitcoin.tax. It will ask if you have a 1099-B, click yes, and then upload the .txf file from Bitcoin.tax. This will automatically upload all of your trades to Turbotax, and you should be good to go. If it asks you to provide a name for the financial institution through which these assets were acquired, I put \"No Financial Institution.\"\\n\\n5. If you are worried in any way about being audited, Turbotax does offer an Audit Protection service for $45 bucks. If you are audited, Turbotax will take care of all of it.\\n\\nA couple of things to keep in mind: \\n\\n1. It is not the case that you are only taxed when you cash out to fiat. You are only taxed on a percentage of your realized gains, meaning a percentage of the profits from the sale of any crypto. This means when you sell something for BTC, you will be taxed on the sale of whatever crypto you sold for BTC, and then any gains made by the BTC while you hold it waiting to buy something else.\\n\\n2. Filing this year may save your ass next year. Filing an amended return is a nightmare, and then paying tax penalties is even worse. Penalties can be up to 75% in some cases. If you need some extra time, file for an extension. It\\'s likely that the IRS will continue to devote more time and resources towards catching those who do not file their taxes on crypto. \\n\\n3. Bitcoin.tax does not seem to work well for everyone, and this is in no way a definitive guide.\\n\\nIf you have any questions I can try and answer them, although I may not be of much help. Thank you and I wish you all the best of luck!\\n\\nIf you found this helpful and are planning to use bitcoin.tax, please consider using my referral code...\\nhttps://bitcoin.tax/r/P3QXCQVf', 'parent_comment_id': 't1_du5x12l', 'comment_id': 'du60v6i', 'comment_body': 'I have the same query. Anyone care to answer? '}, {'score': 5.697470188140869, 'subreddit': 'Monero', 'post_title': '\"Privacy matters\" starter pack', 'post_body': '', 'parent_comment_id': 't1_dr6q7k1', 'comment_id': 'dr6y5xz', 'comment_body': \"It's not really fine, they'll still query your searches...\"}]\n"
     ]
    }
   ],
   "source": [
    "reddit_data = load_reddit_data('./reddit_comments_duplicates_final.json')\n",
    "lucene.initVM(vmargs=['-Djava.awt.headless=true'])\n",
    "create_index('reddit_lucene_index_copy/', reddit_data)\n",
    "results = retrieve('reddit_lucene_index_copy/', 'search query here')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7abfc828-3511-4e1c-be4d-f6aea5b53dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 100 documents. Average time per document: 5.214929580688476e-05Â seconds\n",
      "Indexed 0 documents. Average time per document: 0Â seconds\n",
      "Indexed 0 documents. Average time per document: 0Â seconds\n",
      "Indexed 0 documents. Average time per document: 0Â seconds\n"
     ]
    }
   ],
   "source": [
    "document_counts = [100, 500, 1000, 5000] \n",
    "average_times = []\n",
    "reddit_data = load_reddit_data('./reddit_comments_duplicates_final.json')\n",
    "for count in document_counts:\n",
    "    subset = list(reddit_data)[:count]  \n",
    "    avg_time, total_time, num_docs = create_index('reddit_lucene_index/', subset)\n",
    "    average_times.append(avg_time)\n",
    "    print(f\"Indexed {num_docs} documents. Average time per document: {avg_time}Â seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
